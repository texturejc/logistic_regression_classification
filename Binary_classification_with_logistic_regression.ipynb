{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5300230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14792d7b",
   "metadata": {},
   "source": [
    "# Machine learning with categorical data\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Linear regression based machine learning methods are used to predict numerical outputs. That is, based on the paramteters estimated from its training data, the algorithm takes a series of inputs and predicts the estimated value of their output. However, whilst this method is very useful, there are many situations where we want to predict a *category* rather than a number. For example, we may wish to estimate whether the data predicts whether or not test is passed based on hours studied, or what species of animal is represented based on features in an image, or whether a customer is likely to make a purchase based on their browsing history. This is like regression modelling, except that the numbers in the data are used to predict a categorical rather than a numerical output.\n",
    "\n",
    "Several machine learning algorithms exist for dealing with this situation. We're going to look at one of these––logistic regression––and explore how it can be used for predicting binary categories. This is when we have two outputs (pass or fail, cat or dog, sale or not-sale), with these outcomes represented as $0$ and $1$. It can also be adapted to deal with more than two categories, but we won't be looking at that case.\n",
    "\n",
    "The logic of binary logistic regression works as follows. The model uses a function called the logit function to map the outputs of a regression into the range $(0,1)$. This is useful, because we can then interpret the results of the regression model as probabilities: if they are greater than $0.5$, the model predicts success; if they are less, the model predicts failure. In more detail, the logit function has the following form:\n",
    "\n",
    "$$p(x) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "As you can see, the logit function compresses its output into the $0$ to $1$ range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63b0b67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t = np.arange(-10, 10, 0.1)\n",
    "\n",
    "def logit(x):\n",
    "    y = 1/(1+np.exp(-x))\n",
    "    return y\n",
    "\n",
    "sns.lineplot(x = t, y = [logit(i) for i in t])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89add6fd",
   "metadata": {},
   "source": [
    "## Logistic regression and linear regression\n",
    "\n",
    "The key to understanding logistic regression comes with interpreting the parameter $t$. In logistic regression with a predictor variable $x$ and a predicted variable $y$, this corresponds to the equation for linear regression:\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x$$\n",
    "\n",
    "where $\\beta_{0}$ is the $y$ intercept and $\\beta_{1}$ is the slope of the line of best fit.\n",
    "\n",
    "That is:\n",
    "\n",
    "$$p(x) = \\frac{1}{1+e^{-(\\beta_{0} + \\beta_{1}x)}}$$\n",
    "\n",
    "But how should we interpret this? To understand what's happening, we need to know about *log odds*. The log odds of an event is simply the logarithm of probility of that event occurring divided by the probability of it not occurring. That is, the log odds of an event of proability $p$ is:\n",
    "\n",
    "$$\\ln{\\frac{p}{1-p}}$$\n",
    "\n",
    "Imagine that in a class of students, the probability of a student passing an exam after attending 90% of classes is 0.75. The log odds of a student passing is therefore:\n",
    "\n",
    "$$\\ln{\\frac{0.75}{0.25}} = 1.09$$\n",
    "\n",
    "If this number is large, there is a high probability of success; if it is small, there is a low probability of success.\n",
    "\n",
    "For every value taken by the predictor variable $x$ (the percentage of classes attended) the log odds of a student passing can be calcualted. And because the log odds come in the form of a continuous number than a category, they can then be estimated using a standard linear regression model:\n",
    "\n",
    "$$y =\\ln{\\frac{p(1)}{p(0)}} = \\beta_{0} + \\beta_{1}x$$\n",
    "\n",
    "where $1$ represents passing the exam and $0$ represents failing. By estimating the $\\beta_{0}$ and $\\beta_{1}$ parameters and inserting them into the logit function, we can therefore 'squash' our regression outputs into the (0,1) range, and interpret the results as probabilities. When greater than 0.5, the event is classed as one category (1 = pass) and less than 0.5 it's classed as another (0 = fail). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8277c491",
   "metadata": {},
   "source": [
    "## Evaluating model performance on categorical data\n",
    "\n",
    "With linear regression, the performance of the model is assessed using $R^2$, which gives the goodness of model fit. This doesn't work for categorical data, because the model outputs are categories rather than numbers. Four metrics are typically used to assess model performance in this case: *precision*, *recall*, *accuracy*, and the *F1 score*. These all take values between $0$ and $1$, where $1$ is a perfect score.\n",
    "\n",
    "1. Precision: The precision score of a model measures how many of the positive predictions made by the model are in truth positive predictions. It's calculated as follows, where $TP$ means 'True Positive' and $FP$ means 'False Positive':\n",
    "\n",
    "$$Precision = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "2. Recall: The recall score of a model measures what fraction of the true positive cases the model manages to predict. It's calculated as follows, where $FN$ means 'False Negative':\n",
    "\n",
    "$$Recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "3. Accuracy: The accuracy score of model measures what fraction of correct predictions the model makes relative to the total number of predictions it makes. It's calculated as follows:\n",
    "\n",
    "$$Accuracy = \\frac{TN+TP}{TN+FP+TP+FN}$$\n",
    "\n",
    "4. F1: The F1 score of a model is the harmonic mean of precision and recall. It's used because it gives a way of capturing both metrics in a single score. It's calculated as follows:\n",
    "\n",
    "$$F1 = \\frac{2TP}{2TP+FP+FN}$$\n",
    "\n",
    "Which metric is chosen to evaluate the model depends on the purpose it's being used for. But in practice, the $F1$ is usually the best measure to use, as it gives a more rounded appreciation of model performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f389da",
   "metadata": {},
   "source": [
    "## Logistic regression and NLP\n",
    "\n",
    "There are lots of situations in NLP where logistic regression is useful. For instance, we might want to classify whether or not an email is spam based on the language used, or classify comments as being toxic or not. Here, we're going to build a profanity detector. That is, we will train a logistic regression classifier to evaluate whether or not a word is a profanity, with 'profanity' being understood to include terms of racial, sexual, religious, and other forms of abuse. We will do this in the following way:\n",
    "\n",
    "1. Obtain a list of profanity words and non-profanity words and label them, where $1$ denotes a profanity and $0$ a non-profanity.\n",
    "2. Get word embeddings for these words using a pre-trained model from Twitter.\n",
    "3. Train a logistic regression model on a fraction of this dataset.\n",
    "4. Evaluate our model's performance on a the retained test sample.\n",
    "5. See how our model performs in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3093fb",
   "metadata": {},
   "source": [
    "### 1. Getting our database of words\n",
    "\n",
    "We will take our list of profanity words from the [Surge AI github repo](https://github.com/surge-ai/profanity). **Trigger Warning**: this list is comprehensive and you will find many offensive terms it. Do not read it if you are likely to find it upsetting. We will take our non-profanity words as a random sample from our VAD dataset. Because the Surge AI list has about 500 words and we want our training data to be balanced, the non-profanity sample will also be 500 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0455c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = pd.read_csv('/Users/jamescarney/profanity/profanity_en.csv') #opens our list of profanity words\n",
    "\n",
    "vad = pd.read_csv('/Users/jamescarney/Desktop/VAD_API/Warriner_rescale.csv', index_col = 0) #Loads our VAD data\n",
    "vad = vad[['V.Mean.Sum', 'A.Mean.Sum', 'D.Mean.Sum']]\n",
    "vad.columns = ['valence', 'arousal', 'dominance']good_words = [i for i in vad.index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078461d9",
   "metadata": {},
   "source": [
    "### 2. Load our pre-trained model\n",
    "\n",
    "We will use a model pre-trained on Twitter to get our word embeddings. These embeddings are what are known as `GloVe` vectors; the are similar to `word2vec` vectors but are captured using a different algorithm. Once we've done this, we will pull out the word-embedding vectors for our profanity and non-profanity words and combine the results into a dataframe. We will create a `label` column in the dataframe that assigns $1$ to a profanity word and $0$ to a non-profanity word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9f9aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"glove-twitter-200\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd9d9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_words = [i for i in vad.index]\n",
    "good_words = random.sample(good_words, 500)\n",
    "\n",
    "good_words_ = []\n",
    "good_word_vectors = []\n",
    "\n",
    "for i in good_words:\n",
    "    try:\n",
    "        good_word_vectors.append(model[i])\n",
    "        good_words_.append(i)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "bad_words = []\n",
    "bad_word_vectors = []\n",
    "\n",
    "for i in prof['text']:\n",
    "    try:\n",
    "        bad_word_vectors.append(model[i])\n",
    "        bad_words.append(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce2e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity = pd.DataFrame()\n",
    "profanity['words'] = bad_words\n",
    "profanity['label'] = 1\n",
    "bad_vecs = pd.DataFrame(bad_word_vectors)\n",
    "profanity = pd.concat([profanity, bad_vecs], axis = 1)\n",
    "\n",
    "not_prof = pd.DataFrame()\n",
    "not_prof['words'] = good_words_\n",
    "not_prof['label'] = 0\n",
    "good_vecs = pd.DataFrame(good_word_vectors)\n",
    "not_prof = pd.concat([not_prof, good_vecs], axis = 1)\n",
    "\n",
    "data = pd.concat([profanity, not_prof]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d80cf442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abbie</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.329500</td>\n",
       "      <td>0.085633</td>\n",
       "      <td>0.36949</td>\n",
       "      <td>-0.074677</td>\n",
       "      <td>-0.69475</td>\n",
       "      <td>0.044499</td>\n",
       "      <td>-0.083036</td>\n",
       "      <td>-0.017181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.527830</td>\n",
       "      <td>0.408630</td>\n",
       "      <td>0.105730</td>\n",
       "      <td>-0.430270</td>\n",
       "      <td>-0.483210</td>\n",
       "      <td>-0.709040</td>\n",
       "      <td>0.717630</td>\n",
       "      <td>-0.048857</td>\n",
       "      <td>0.187110</td>\n",
       "      <td>-0.464220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abeed</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.485850</td>\n",
       "      <td>0.419280</td>\n",
       "      <td>-0.27153</td>\n",
       "      <td>-0.122530</td>\n",
       "      <td>-0.91392</td>\n",
       "      <td>-0.391950</td>\n",
       "      <td>-0.586190</td>\n",
       "      <td>0.554930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017666</td>\n",
       "      <td>-0.365510</td>\n",
       "      <td>-0.099251</td>\n",
       "      <td>0.658930</td>\n",
       "      <td>-0.741040</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.844530</td>\n",
       "      <td>0.066441</td>\n",
       "      <td>-0.326710</td>\n",
       "      <td>-0.799120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aboe</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.239690</td>\n",
       "      <td>0.249420</td>\n",
       "      <td>-0.29688</td>\n",
       "      <td>-0.024667</td>\n",
       "      <td>-0.32035</td>\n",
       "      <td>-0.394760</td>\n",
       "      <td>-0.308500</td>\n",
       "      <td>-0.711500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406490</td>\n",
       "      <td>0.290960</td>\n",
       "      <td>-0.398580</td>\n",
       "      <td>-0.107710</td>\n",
       "      <td>0.218790</td>\n",
       "      <td>-0.137130</td>\n",
       "      <td>0.569850</td>\n",
       "      <td>0.024671</td>\n",
       "      <td>-0.981870</td>\n",
       "      <td>0.235720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anal</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.776660</td>\n",
       "      <td>-0.323780</td>\n",
       "      <td>0.65202</td>\n",
       "      <td>-0.045757</td>\n",
       "      <td>-0.11865</td>\n",
       "      <td>-0.405230</td>\n",
       "      <td>0.466450</td>\n",
       "      <td>0.234970</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204910</td>\n",
       "      <td>-0.720360</td>\n",
       "      <td>-0.489530</td>\n",
       "      <td>0.056852</td>\n",
       "      <td>1.146400</td>\n",
       "      <td>-0.106290</td>\n",
       "      <td>0.248590</td>\n",
       "      <td>0.141540</td>\n",
       "      <td>-0.739220</td>\n",
       "      <td>-0.070684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analingus</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.650990</td>\n",
       "      <td>-0.783900</td>\n",
       "      <td>0.31116</td>\n",
       "      <td>-0.515960</td>\n",
       "      <td>-0.26272</td>\n",
       "      <td>-0.411900</td>\n",
       "      <td>0.414180</td>\n",
       "      <td>-0.218250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615030</td>\n",
       "      <td>-0.757560</td>\n",
       "      <td>0.659290</td>\n",
       "      <td>0.312920</td>\n",
       "      <td>-0.055653</td>\n",
       "      <td>-0.398210</td>\n",
       "      <td>0.028032</td>\n",
       "      <td>0.175700</td>\n",
       "      <td>-0.675020</td>\n",
       "      <td>-0.090851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>employment</td>\n",
       "      <td>0</td>\n",
       "      <td>0.396370</td>\n",
       "      <td>-0.175640</td>\n",
       "      <td>-0.67779</td>\n",
       "      <td>-0.296250</td>\n",
       "      <td>0.31742</td>\n",
       "      <td>0.054992</td>\n",
       "      <td>0.714460</td>\n",
       "      <td>-0.302520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647930</td>\n",
       "      <td>-0.811810</td>\n",
       "      <td>0.668000</td>\n",
       "      <td>-0.459320</td>\n",
       "      <td>-0.020810</td>\n",
       "      <td>-0.780850</td>\n",
       "      <td>0.197580</td>\n",
       "      <td>0.051412</td>\n",
       "      <td>0.287660</td>\n",
       "      <td>-0.707980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>innocence</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.028419</td>\n",
       "      <td>-0.163720</td>\n",
       "      <td>-0.34169</td>\n",
       "      <td>0.601560</td>\n",
       "      <td>0.32881</td>\n",
       "      <td>0.352210</td>\n",
       "      <td>0.371940</td>\n",
       "      <td>-0.601690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.691960</td>\n",
       "      <td>0.090765</td>\n",
       "      <td>-0.539420</td>\n",
       "      <td>0.973210</td>\n",
       "      <td>0.743630</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.058157</td>\n",
       "      <td>-0.184260</td>\n",
       "      <td>-0.721350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>platter</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.309040</td>\n",
       "      <td>0.050861</td>\n",
       "      <td>0.12084</td>\n",
       "      <td>-0.193310</td>\n",
       "      <td>0.17790</td>\n",
       "      <td>0.374320</td>\n",
       "      <td>0.145030</td>\n",
       "      <td>0.021865</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080114</td>\n",
       "      <td>-0.013020</td>\n",
       "      <td>0.520150</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>-0.041028</td>\n",
       "      <td>0.932000</td>\n",
       "      <td>0.380190</td>\n",
       "      <td>0.274880</td>\n",
       "      <td>-0.074193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>courteous</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.004091</td>\n",
       "      <td>0.244950</td>\n",
       "      <td>0.33981</td>\n",
       "      <td>0.662490</td>\n",
       "      <td>0.39993</td>\n",
       "      <td>0.336050</td>\n",
       "      <td>0.104200</td>\n",
       "      <td>-0.536210</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.416560</td>\n",
       "      <td>0.370360</td>\n",
       "      <td>0.162440</td>\n",
       "      <td>-0.357850</td>\n",
       "      <td>0.204060</td>\n",
       "      <td>-0.440760</td>\n",
       "      <td>0.383750</td>\n",
       "      <td>0.412020</td>\n",
       "      <td>-0.924990</td>\n",
       "      <td>-0.586510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>picnic</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.826870</td>\n",
       "      <td>0.252460</td>\n",
       "      <td>0.84174</td>\n",
       "      <td>0.136070</td>\n",
       "      <td>1.06090</td>\n",
       "      <td>-0.226000</td>\n",
       "      <td>0.303330</td>\n",
       "      <td>0.108570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832810</td>\n",
       "      <td>-0.053010</td>\n",
       "      <td>0.681340</td>\n",
       "      <td>-0.356710</td>\n",
       "      <td>-0.157540</td>\n",
       "      <td>0.345010</td>\n",
       "      <td>0.482820</td>\n",
       "      <td>-0.323000</td>\n",
       "      <td>-0.024801</td>\n",
       "      <td>0.032859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>992 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  label         0         1        2         3        4  \\\n",
       "0         abbie      1 -0.329500  0.085633  0.36949 -0.074677 -0.69475   \n",
       "1         abeed      1 -0.485850  0.419280 -0.27153 -0.122530 -0.91392   \n",
       "2          aboe      1 -0.239690  0.249420 -0.29688 -0.024667 -0.32035   \n",
       "3          anal      1 -0.776660 -0.323780  0.65202 -0.045757 -0.11865   \n",
       "4     analingus      1 -0.650990 -0.783900  0.31116 -0.515960 -0.26272   \n",
       "..          ...    ...       ...       ...      ...       ...      ...   \n",
       "987  employment      0  0.396370 -0.175640 -0.67779 -0.296250  0.31742   \n",
       "988   innocence      0 -0.028419 -0.163720 -0.34169  0.601560  0.32881   \n",
       "989     platter      0 -0.309040  0.050861  0.12084 -0.193310  0.17790   \n",
       "990   courteous      0 -0.004091  0.244950  0.33981  0.662490  0.39993   \n",
       "991      picnic      0 -0.826870  0.252460  0.84174  0.136070  1.06090   \n",
       "\n",
       "            5         6         7  ...       190       191       192  \\\n",
       "0    0.044499 -0.083036 -0.017181  ... -0.527830  0.408630  0.105730   \n",
       "1   -0.391950 -0.586190  0.554930  ... -0.017666 -0.365510 -0.099251   \n",
       "2   -0.394760 -0.308500 -0.711500  ...  0.406490  0.290960 -0.398580   \n",
       "3   -0.405230  0.466450  0.234970  ...  0.204910 -0.720360 -0.489530   \n",
       "4   -0.411900  0.414180 -0.218250  ...  0.615030 -0.757560  0.659290   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "987  0.054992  0.714460 -0.302520  ...  0.647930 -0.811810  0.668000   \n",
       "988  0.352210  0.371940 -0.601690  ...  0.691960  0.090765 -0.539420   \n",
       "989  0.374320  0.145030  0.021865  ... -0.080114 -0.013020  0.520150   \n",
       "990  0.336050  0.104200 -0.536210  ... -0.416560  0.370360  0.162440   \n",
       "991 -0.226000  0.303330  0.108570  ...  0.832810 -0.053010  0.681340   \n",
       "\n",
       "          193       194       195       196       197       198       199  \n",
       "0   -0.430270 -0.483210 -0.709040  0.717630 -0.048857  0.187110 -0.464220  \n",
       "1    0.658930 -0.741040  0.261200  0.844530  0.066441 -0.326710 -0.799120  \n",
       "2   -0.107710  0.218790 -0.137130  0.569850  0.024671 -0.981870  0.235720  \n",
       "3    0.056852  1.146400 -0.106290  0.248590  0.141540 -0.739220 -0.070684  \n",
       "4    0.312920 -0.055653 -0.398210  0.028032  0.175700 -0.675020 -0.090851  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "987 -0.459320 -0.020810 -0.780850  0.197580  0.051412  0.287660 -0.707980  \n",
       "988  0.973210  0.743630  0.025837  0.029231  0.058157 -0.184260 -0.721350  \n",
       "989  0.132700  0.531500 -0.041028  0.932000  0.380190  0.274880 -0.074193  \n",
       "990 -0.357850  0.204060 -0.440760  0.383750  0.412020 -0.924990 -0.586510  \n",
       "991 -0.356710 -0.157540  0.345010  0.482820 -0.323000 -0.024801  0.032859  \n",
       "\n",
       "[992 rows x 202 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d48dae",
   "metadata": {},
   "source": [
    "### 3. Train a logistic regression classifier on our data\n",
    "\n",
    "This procedure is more-or-less the same as the procedure for linear regression, where we withhold a fraction of the data for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91608758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['words', 'label'], axis = 'columns')\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19be75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train) #This fits the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9873e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test) # This outputs the model predictions on the withheld test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a04093",
   "metadata": {},
   "source": [
    "### 4. Evaluate our model on the retained test data\n",
    "\n",
    "We can evaluate the model on all of our four evaluation metrics if we like, but the `scikit-learn` `classification_report` function gives a useful summary of all four."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52418654",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ccc5b5",
   "metadata": {},
   "source": [
    "### 5. Test our model in the wild\n",
    "\n",
    "Having trained our model, it should now be able to classify words in the pre-trained model that are not in the profanity list. It should also be able to distinguish between words that are merely *unpleasant* rather than offensive. (Note that the Twitter vocabulary has all been lowercased, so queried words should also be lowercased.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "996d0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = ['snot', 'scum', 'turd', 'piss', 'commie', 'nazi', 'bloody', 'peasant', 'bigot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f11944dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "\n",
    "for i in test_cases:\n",
    "    tests.append(clf.predict(model[i.lower()].reshape(1, -1))[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825e25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
